===next3_snapshot_race_cow.patch===

next3: snapshot race conditions - concurrent COW operations

Wait for pending COW operations to complete.
When concurrent tasks try to COW the same buffer, the task that takes
the active snapshot truncate_mutex is elected as the the COWing task.
The COWing task allocates a new snapshot block and creates a buffer
cache entry with ref_count=1 for that new block.  It then locks the
new buffer and marks it with the buffer_new flag.  The rest of the
tasks wait (in msleep(1) loop), until the buffer_new flag is cleared.
The COWing task copies the source buffer into the 'new' buffer,
unlocks it, marks it as journal dirty data, clears the new_buffer flag
and drops its reference count.
On active snapshot readpage, the buffer cache is checked.
If a 'new' buffer entry is found, the reader task waits until the
buffer_new flag is cleared and then copies the 'new' buffer directly
into the snapshot file page.

Signed-off-by: Amir Goldstein <amir73il@users.sf.net>

--------------------------------------------------------------------------------
diff -Nuarp a/fs/next3/inode.c b/fs/next3/inode.c
--- a/fs/next3/inode.c	2010-07-26 09:21:53.154779689 +0300
+++ b/fs/next3/inode.c	2010-07-26 09:21:52.564771742 +0300
@@ -882,6 +882,7 @@ int next3_get_blocks_handle(handle_t *ha
 	next3_fsblk_t first_block = 0;
 	int read_through = 0;
 	struct inode *prev_snapshot;
+	struct buffer_head *sbh = NULL;
 
 retry:
 	blocks_to_boundary = 0;
@@ -1071,6 +1072,25 @@ retry:
 	if (err)
 		goto out_mutex;
 
+	if (SNAPMAP_ISCOW(create)) {
+		/*
+		 * COWing block or creating COW bitmap.
+		 * we now have exclusive access to the COW destination block
+		 * and we are about to create the snapshot block mapping
+		 * and make it public.
+		 * grab the buffer cache entry and mark it new
+		 * to indicate a pending COW operation.
+		 * the refcount for the buffer cache will be released
+		 * when the COW operation is either completed or canceled.
+		 */
+		sbh = sb_getblk(inode->i_sb, le32_to_cpu(chain[depth-1].key));
+		if (!sbh) {
+			err = -EIO;
+			goto out_mutex;
+		}
+		next3_snapshot_start_pending_cow(sbh);
+	}
+
 	if (*(partial->p)) {
 		int ret;
 
@@ -1126,12 +1146,54 @@ out_mutex:
 	set_buffer_new(bh_result);
 got_it:
 	map_bh(bh_result, inode->i_sb, le32_to_cpu(chain[depth-1].key));
+	/*
+	 * On read of active snapshot, a mapped block may belong to a non
+	 * completed COW operation.  Use the buffer cache to test this
+	 * condition.  if (bh_result->b_blocknr == SNAPSHOT_BLOCK(iblock)),
+	 * then this is either read through to block device or moved block.
+	 * Either way, it is not a COWed block, so it cannot be pending COW.
+	 */
+	if (read_through && next3_snapshot_is_active(inode) &&
+		bh_result->b_blocknr != SNAPSHOT_BLOCK(iblock))
+		sbh = sb_find_get_block(inode->i_sb, bh_result->b_blocknr);
+	if (read_through && sbh) {
+		/* wait for pending COW to complete */
+		next3_snapshot_test_pending_cow(sbh, SNAPSHOT_BLOCK(iblock));
+		lock_buffer(sbh);
+		if (buffer_uptodate(sbh)) {
+			/*
+			 * Avoid disk I/O and copy out snapshot page directly
+			 * from block device page when possible.
+			 */
+			BUG_ON(!sbh->b_page);
+			BUG_ON(!bh_result->b_page);
+			lock_buffer(bh_result);
+			copy_highpage(bh_result->b_page, sbh->b_page);
+			set_buffer_uptodate(bh_result);
+			unlock_buffer(bh_result);
+		} else if (buffer_dirty(sbh)) {
+			/*
+			 * If snapshot data buffer is dirty (just been COWed),
+			 * then it is not safe to read it from disk yet.
+			 * We shouldn't get here because snapshot data buffer
+			 * only becomes dirty during COW and because we waited
+			 * for pending COW to complete, which means that a
+			 * dirty snapshot data buffer should be uptodate.
+			 */
+			WARN_ON(1);
+		}
+		unlock_buffer(sbh);
+	}
 	if (count > blocks_to_boundary)
 		set_buffer_boundary(bh_result);
 	err = count;
 	/* Clean up and exit */
 	partial = chain + depth - 1;	/* the whole chain */
 cleanup:
+	/* cancel pending COW operation on failure to alloc snapshot block */
+	if (create && err < 0 && sbh)
+		next3_snapshot_end_pending_cow(sbh);
+	brelse(sbh);
 	while (partial > chain) {
 		BUFFER_TRACE(partial->bh, "call brelse");
 		brelse(partial->bh);
diff -Nuarp a/fs/next3/snapshot.c b/fs/next3/snapshot.c
--- a/fs/next3/snapshot.c	2010-07-26 09:21:53.234863161 +0300
+++ b/fs/next3/snapshot.c	2010-07-26 09:21:52.644772832 +0300
@@ -249,6 +249,8 @@ next3_snapshot_complete_cow(handle_t *ha
 		sync_dirty_buffer(sbh);
 
 out:
+	/* COW operation is complete */
+	next3_snapshot_end_pending_cow(sbh);
 	return err;
 }
 
@@ -866,6 +868,13 @@ int next3_snapshot_test_and_cow(const ch
 	 * we allocated this block -
 	 * copy block data to snapshot and complete COW operation
 	 */
+	snapshot_debug(3, "COWing block [%lu/%lu] of snapshot "
+			"(%u)...\n",
+			SNAPSHOT_BLOCK_GROUP_OFFSET(block),
+			SNAPSHOT_BLOCK_GROUP(block),
+			active_snapshot->i_generation);
+	/* sleep 1 tunable delay unit */
+	snapshot_test_delay(SNAPTEST_COW);
 	err = next3_snapshot_copy_buffer_cow(handle, sbh, bh);
 	if (err)
 		goto out;
@@ -877,6 +886,9 @@ int next3_snapshot_test_and_cow(const ch
 
 	trace_cow_inc(handle, copied);
 test_pending_cow:
+	if (sbh)
+		/* wait for pending COW to complete */
+		next3_snapshot_test_pending_cow(sbh, block);
 
 cowed:
 	/* mark the buffer COWed in the current transaction */
diff -Nuarp a/fs/next3/snapshot.h b/fs/next3/snapshot.h
--- a/fs/next3/snapshot.h	2010-07-26 09:21:53.244782753 +0300
+++ b/fs/next3/snapshot.h	2010-07-26 09:21:52.664772116 +0300
@@ -396,6 +396,71 @@ static inline int next3_snapshot_is_acti
 	return (inode == NEXT3_SB(inode->i_sb)->s_active_snapshot);
 }
 
+/*
+ * Pending COW functions
+ */
+
+/*
+ * Start pending COW operation from get_blocks_handle()
+ * after allocating snapshot block and before connecting it
+ * to the snapshot inode.
+ */
+static inline void next3_snapshot_start_pending_cow(struct buffer_head *sbh)
+{
+	/*
+	 * setting the 'new' flag on a newly allocated snapshot block buffer
+	 * indicates that the COW operation is pending.
+	 */
+	set_buffer_new(sbh);
+	/* keep buffer in cache as long as we need to test the 'new' flag */
+	get_bh(sbh);
+}
+
+/*
+ * End pending COW operation started in get_blocks_handle().
+ * Called on failure to connect the new snapshot block to the inode
+ * or on successful completion of the COW operation.
+ */
+static inline void next3_snapshot_end_pending_cow(struct buffer_head *sbh)
+{
+	/*
+	 * clearing the 'new' flag from the snapshot block buffer
+	 * indicates that the COW operation is complete.
+	 */
+	clear_buffer_new(sbh);
+	/* we no longer need to keep the buffer in cache */
+	put_bh(sbh);
+}
+
+/*
+ * Test for pending COW operation and wait for its completion.
+ */
+static inline void next3_snapshot_test_pending_cow(struct buffer_head *sbh,
+						sector_t blocknr)
+{
+	SNAPSHOT_DEBUG_ONCE;
+	while (buffer_new(sbh)) {
+		/* wait for pending COW to complete */
+		snapshot_debug_once(2, "waiting for pending cow: "
+				"block = [%lu/%lu]...\n",
+				SNAPSHOT_BLOCK_TUPLE(blocknr));
+		/*
+		 * An unusually long pending COW operation can be caused by
+		 * the debugging function snapshot_test_delay(SNAPTEST_COW)
+		 * and by waiting for tracked reads to complete.
+		 * The new COW buffer is locked during those events, so wait
+		 * on the buffer before the short msleep.
+		 */
+		wait_on_buffer(sbh);
+		/*
+		 * This is an unlikely event that can happen only once per
+		 * block/snapshot, so msleep(1) is sufficient and there is
+		 * no need for a wait queue.
+		 */
+		msleep(1);
+		/* XXX: Should we fail after N retries? */
+	}
+}
 
 
 #endif	/* _LINUX_NEXT3_SNAPSHOT_H */
