===next3_snapshot_race_read.patch===

next3: snapshot race conditions - tracked reads

Wait for pending read I/O requests to complete.
When a snapshot file readpage reads through to the block device,
the reading task increments the block tracked readers count.
Upon completion of the async read I/O request of the snapshot page,
the tracked readers count is decremented.
When a task is COWing a block with non-zero tracked readers count,
that task has to wait (in msleep(1) loop), until the block's tracked
readers count drops to zero, before the COW operation is completed.
After a pending COW operation has started, reader tasks have to wait
(again, in msleep(1) loop), until the pending COW operation is
completed, so the COWing task cannot be starved by reader tasks.

Signed-off-by: Amir Goldstein <amir73il@users.sf.net>

--------------------------------------------------------------------------------
diff -Nuarp a/fs/next3/buffer.c b/fs/next3/buffer.c
--- a/fs/next3/buffer.c	2010-07-26 09:21:52.644772832 +0300
+++ b/fs/next3/buffer.c	2010-07-26 09:21:52.094828942 +0300
@@ -66,6 +66,119 @@ static void buffer_io_error(struct buffe
 }
 
 /*
+ * Tracked read functions.
+ * When reading through a next3 snapshot file hole to a block device block,
+ * all writes to this block need to wait for completion of the async read.
+ * next3_snapshot_readpage() always calls next3_read_full_page() to attach
+ * a buffer head to the page and be aware of tracked reads.
+ * next3_snapshot_get_block() calls start_buffer_tracked_read() to mark both
+ * snapshot page buffer and block device page buffer.
+ * next3_snapshot_get_block() calls cancel_buffer_tracked_read() if snapshot
+ * doesn't need to read through to the block device.
+ * next3_read_full_page() calls submit_buffer_tracked_read() to submit a
+ * tracked async read.
+ * end_buffer_async_read() calls end_buffer_tracked_read() to complete the
+ * tracked read operation.
+ * The only lock needed in all these functions is PageLock on the snapshot page,
+ * which is guarantied in readpage() and verified in next3_read_full_page().
+ * The block device page buffer doesn't need any lock because the operations
+ * {get|put}_bh_tracked_reader() are atomic.
+ */
+
+/*
+ * start buffer tracked read
+ * called from inside get_block()
+ * get tracked reader ref count on buffer cache entry
+ * and set buffer tracked read flag
+ */
+int start_buffer_tracked_read(struct buffer_head *bh)
+{
+	struct buffer_head *bdev_bh;
+
+	BUG_ON(buffer_tracked_read(bh));
+	BUG_ON(!buffer_mapped(bh));
+
+	/* grab the buffer cache entry */
+	bdev_bh = __getblk(bh->b_bdev, bh->b_blocknr, bh->b_size);
+	if (!bdev_bh)
+		return -EIO;
+
+	BUG_ON(bdev_bh == bh);
+	set_buffer_tracked_read(bh);
+	get_bh_tracked_reader(bdev_bh);
+	put_bh(bdev_bh);
+	return 0;
+}
+
+/*
+ * cancel buffer tracked read
+ * called for tracked read that was started but was not submitted
+ * put tracked reader ref count on buffer cache entry
+ * and clear buffer tracked read flag
+ */
+void cancel_buffer_tracked_read(struct buffer_head *bh)
+{
+	struct buffer_head *bdev_bh;
+
+	BUG_ON(!buffer_tracked_read(bh));
+	BUG_ON(!buffer_mapped(bh));
+
+	/* try to grab the buffer cache entry */
+	bdev_bh = __find_get_block(bh->b_bdev, bh->b_blocknr, bh->b_size);
+	BUG_ON(!bdev_bh || bdev_bh == bh);
+	clear_buffer_tracked_read(bh);
+	clear_buffer_mapped(bh);
+	put_bh_tracked_reader(bdev_bh);
+	put_bh(bdev_bh);
+}
+
+/*
+ * submit buffer tracked read
+ * save a reference to buffer cache entry and submit I/O
+ */
+static int submit_buffer_tracked_read(struct buffer_head *bh)
+{
+	struct buffer_head *bdev_bh;
+	BUG_ON(!buffer_tracked_read(bh));
+	BUG_ON(!buffer_mapped(bh));
+	/* tracked read doesn't work with multiple buffers per page */
+	BUG_ON(bh->b_this_page != bh);
+
+	/*
+	 * Try to grab the buffer cache entry before submitting async read
+	 * because we cannot call blocking function __find_get_block()
+	 * in interrupt context inside end_buffer_tracked_read().
+	 */
+	bdev_bh = __find_get_block(bh->b_bdev, bh->b_blocknr, bh->b_size);
+	BUG_ON(!bdev_bh || bdev_bh == bh);
+	/* override page buffers list with reference to buffer cache entry */
+	bh->b_this_page = bdev_bh;
+	submit_bh(READ, bh);
+	return 0;
+}
+
+/*
+ * end buffer tracked read
+ * complete submitted tracked read
+ */
+static void end_buffer_tracked_read(struct buffer_head *bh)
+{
+	struct buffer_head *bdev_bh = bh->b_this_page;
+
+	BUG_ON(!buffer_tracked_read(bh));
+	BUG_ON(!bdev_bh || bdev_bh == bh);
+	bh->b_this_page = bh;
+	/*
+	 * clear the buffer mapping to make sure
+	 * that get_block() will always be called
+	 */
+	clear_buffer_mapped(bh);
+	clear_buffer_tracked_read(bh);
+	put_bh_tracked_reader(bdev_bh);
+	put_bh(bdev_bh);
+}
+
+/*
  * I/O completion handler for next3_read_full_page() - pages
  * which come unlocked at the end of I/O.
  */
@@ -79,6 +192,9 @@ static void end_buffer_async_read(struct
 
 	BUG_ON(!buffer_async_read(bh));
 
+	if (buffer_tracked_read(bh))
+		end_buffer_tracked_read(bh);
+
 	page = bh->b_page;
 	if (uptodate) {
 		set_buffer_uptodate(bh);
@@ -240,6 +356,8 @@ int next3_read_full_page(struct page *pa
 	 */
 	for (i = 0; i < nr; i++) {
 		bh = arr[i];
+		if (buffer_tracked_read(bh))
+			return submit_buffer_tracked_read(bh);
 		if (buffer_uptodate(bh))
 			end_buffer_async_read(bh, 1);
 		else
diff -Nuarp a/fs/next3/inode.c b/fs/next3/inode.c
--- a/fs/next3/inode.c	2010-07-26 09:21:52.564771742 +0300
+++ b/fs/next3/inode.c	2010-07-26 09:21:52.024760744 +0300
@@ -900,6 +900,25 @@ retry:
 		err = read_through;
 		goto out;
 	}
+	if (read_through && !prev_snapshot) {
+		/*
+		 * Possible read through to block device.
+		 * Start tracked read before checking if block is mapped to
+		 * avoid race condition with COW that maps the block after
+		 * we checked if the block is mapped.  If we find that the
+		 * block is mapped, we will cancel the tracked read before
+		 * returning from this function.
+		 */
+		map_bh(bh_result, inode->i_sb, SNAPSHOT_BLOCK(iblock));
+		err = start_buffer_tracked_read(bh_result);
+		if (err < 0) {
+			snapshot_debug(1, "snapshot (%u) failed to start "
+					"tracked read on block (%lld) "
+					"(err=%d)\n", inode->i_generation,
+					(long long)bh_result->b_blocknr, err);
+			goto out;
+		}
+	}
 	err = -EIO;
 
 
@@ -1145,6 +1164,10 @@ out_mutex:
 
 	set_buffer_new(bh_result);
 got_it:
+	/* it's not a hole - cancel tracked read before we deadlock on
+	 * pending COW */
+	if (buffer_tracked_read(bh_result))
+		cancel_buffer_tracked_read(bh_result);
 	map_bh(bh_result, inode->i_sb, le32_to_cpu(chain[depth-1].key));
 	/*
 	 * On read of active snapshot, a mapped block may belong to a non
@@ -1190,6 +1213,9 @@ got_it:
 	/* Clean up and exit */
 	partial = chain + depth - 1;	/* the whole chain */
 cleanup:
+	/* cancel tracked read on failure to read through active snapshot */
+	if (read_through && err < 0 && buffer_tracked_read(bh_result))
+		cancel_buffer_tracked_read(bh_result);
 	/* cancel pending COW operation on failure to alloc snapshot block */
 	if (create && err < 0 && sbh)
 		next3_snapshot_end_pending_cow(sbh);
@@ -1985,6 +2011,90 @@ out_unlock:
 	goto out;
 }
 
+static int next3_snapshot_get_block(struct inode *inode, sector_t iblock,
+			struct buffer_head *bh_result, int create)
+{
+	unsigned long block_group;
+	struct next3_group_desc *desc;
+	struct next3_group_info *gi;
+	next3_fsblk_t bitmap_blk = 0;
+	int err;
+
+	BUG_ON(create != 0);
+	BUG_ON(buffer_tracked_read(bh_result));
+
+	err = next3_get_blocks_handle(NULL, inode, SNAPSHOT_IBLOCK(iblock),
+					1, bh_result, 0);
+
+	snapshot_debug(4, "next3_snapshot_get_block(%lld): block = (%lld), "
+			"err = %d\n",
+			(long long)iblock, buffer_mapped(bh_result) ?
+			(long long)bh_result->b_blocknr : 0, err);
+
+	if (err < 0)
+		return err;
+
+	if (!buffer_tracked_read(bh_result))
+		return 0;
+
+	/* check for read through to block bitmap */
+	block_group = SNAPSHOT_BLOCK_GROUP(bh_result->b_blocknr);
+	desc = next3_get_group_desc(inode->i_sb, block_group, NULL);
+	if (desc)
+		bitmap_blk = le32_to_cpu(desc->bg_block_bitmap);
+	if (bitmap_blk && bitmap_blk == bh_result->b_blocknr) {
+		/* copy fixed block bitmap directly to page buffer */
+		cancel_buffer_tracked_read(bh_result);
+		/* cancel_buffer_tracked_read() clears mapped flag */
+		set_buffer_mapped(bh_result);
+		snapshot_debug(2, "fixing snapshot block bitmap #%lu\n",
+				block_group);
+		/*
+		 * XXX: if we return unmapped buffer, the page will be zeroed
+		 * but if we return mapped to block device and uptodate buffer
+		 * next readpage may read directly from block device without
+		 * fixing block bitmap.  This only affects fsck of snapshots.
+		 */
+		return next3_snapshot_read_block_bitmap(inode->i_sb,
+				block_group, bh_result);
+	}
+	/* check for read through to exclude bitmap */
+	gi = NEXT3_SB(inode->i_sb)->s_group_info + block_group;
+	bitmap_blk = gi->bg_exclude_bitmap;
+	if (bitmap_blk && bitmap_blk == bh_result->b_blocknr) {
+		/* return unmapped buffer to zero out page */
+		cancel_buffer_tracked_read(bh_result);
+		/* cancel_buffer_tracked_read() clears mapped flag */
+		snapshot_debug(2, "zeroing snapshot exclude bitmap #%lu\n",
+				block_group);
+		return 0;
+	}
+
+#ifdef CONFIG_NEXT3_FS_DEBUG
+	snapshot_debug(3, "started tracked read: block = [%lu/%lu]\n",
+			SNAPSHOT_BLOCK_TUPLE(bh_result->b_blocknr));
+
+	if (snapshot_enable_test[SNAPTEST_READ]) {
+		err = next3_snapshot_get_read_access(inode->i_sb,
+				bh_result);
+		if (err) {
+			/* read through access denied */
+			cancel_buffer_tracked_read(bh_result);
+			return err;
+		}
+		/* sleep 1 tunable delay unit */
+		snapshot_test_delay(SNAPTEST_READ);
+	}
+#endif
+	return 0;
+}
+
+static int next3_snapshot_readpage(struct file *file, struct page *page)
+{
+	/* do read I/O with buffer heads to enable tracked reads */
+	return next3_read_full_page(page, next3_snapshot_get_block);
+}
+
 static int next3_readpage(struct file *file, struct page *page)
 {
 	return mpage_readpage(page, next3_get_block);
@@ -2192,8 +2302,7 @@ static int next3_no_writepage(struct pag
  * the snapshot COW bitmaps and a few initial blocks copied on snapshot_take().
  */
 static const struct address_space_operations next3_snapfile_aops = {
-	.readpage		= next3_readpage,
-	.readpages		= next3_readpages,
+	.readpage		= next3_snapshot_readpage,
 	.writepage		= next3_no_writepage,
 	.bmap			= next3_bmap,
 	.invalidatepage		= next3_invalidatepage,
diff -Nuarp a/fs/next3/snapshot.c b/fs/next3/snapshot.c
--- a/fs/next3/snapshot.c	2010-07-26 09:21:52.644772832 +0300
+++ b/fs/next3/snapshot.c	2010-07-26 09:21:52.104763798 +0300
@@ -237,6 +237,24 @@ next3_snapshot_complete_cow(handle_t *ha
 		struct buffer_head *sbh, struct buffer_head *bh, int sync)
 {
 	int err = 0;
+	SNAPSHOT_DEBUG_ONCE;
+
+	/* wait for completion of tracked reads before completing COW */
+	while (bh && buffer_tracked_readers_count(bh) > 0) {
+		snapshot_debug_once(2, "waiting for tracked reads: "
+			"block = [%lu/%lu], "
+			"tracked_readers_count = %d...\n",
+			SNAPSHOT_BLOCK_TUPLE(bh->b_blocknr),
+			buffer_tracked_readers_count(bh));
+		/*
+		 * Quote from LVM snapshot pending_complete() function:
+	         * "Check for conflicting reads. This is extremely improbable,
+		 *  so msleep(1) is sufficient and there is no need for a wait
+		 *  queue." (drivers/md/dm-snap.c).
+		 */
+		msleep(1);
+		/* XXX: Should we fail after N retries? */
+	}
 
 	unlock_buffer(sbh);
 	if (handle) {
@@ -1003,3 +1021,44 @@ out:
 	return err;
 }
 
+/*
+ * next3_snapshot_get_read_access - get read through access to block device.
+ * Sanity test to verify that the read block is allocated and not excluded.
+ * This test has performance penalty and is only called if SNAPTEST_READ
+ * is enabled.  An attempt to read through to block device of a non allocated
+ * or excluded block may indicate a corrupted filesystem, corrupted snapshot
+ * or corrupted exclude bitmap.  However, it may also be a read-ahead, which
+ * was not implicitly requested by the user, so be sure to disable read-ahead
+ * on block device (blockdev --setra 0 <bdev>) before enabling SNAPTEST_READ.
+ *
+ * Return values:
+ * = 0 - block is allocated and not excluded
+ * < 0 - error (or block is not allocated or excluded)
+ */
+int next3_snapshot_get_read_access(struct super_block *sb,
+				   struct buffer_head *bh)
+{
+	unsigned long block_group = SNAPSHOT_BLOCK_GROUP(bh->b_blocknr);
+	next3_grpblk_t bit = SNAPSHOT_BLOCK_GROUP_OFFSET(bh->b_blocknr);
+	struct buffer_head *bitmap_bh;
+	int err = 0;
+
+	if (PageReadahead(bh->b_page))
+		return 0;
+
+	bitmap_bh = read_block_bitmap(sb, block_group);
+	if (!bitmap_bh)
+		return -EIO;
+
+	if (!next3_test_bit(bit, bitmap_bh->b_data)) {
+		snapshot_debug(2, "warning: attempt to read through to "
+				"non-allocated block [%d/%lu] - read ahead?\n",
+				bit, block_group);
+		brelse(bitmap_bh);
+		return -EIO;
+	}
+
+
+	brelse(bitmap_bh);
+	return err;
+}
diff -Nuarp a/fs/next3/snapshot.h b/fs/next3/snapshot.h
--- a/fs/next3/snapshot.h	2010-07-26 09:21:52.664772116 +0300
+++ b/fs/next3/snapshot.h	2010-07-26 09:21:52.114833867 +0300
@@ -87,9 +87,14 @@
 	i_size_write((inode), 0)
 
 enum next3_bh_state_bits {
+	BH_Tracked_Read = 30,	/* Buffer read I/O is being tracked,
+							 * to serialize write I/O to block device.
+							 * that is, don't write over this block
+							 * until I finished reading it. */
 	BH_Partial_Write = 31,	/* Buffer should be read before write */
 };
 
+BUFFER_FNS(Tracked_Read, tracked_read)
 BUFFER_FNS(Partial_Write, partial_write)
 
 /*
@@ -272,6 +277,8 @@ static inline int next3_snapshot_get_del
 	return next3_snapshot_move(handle, inode, block, count, 1);
 }
 
+extern int next3_snapshot_get_read_access(struct super_block *sb,
+					  struct buffer_head *bh);
 extern int next3_snapshot_get_inode_access(handle_t *handle,
 					   struct inode *inode,
 					   next3_fsblk_t iblock,
@@ -462,5 +469,33 @@ static inline void next3_snapshot_test_p
 	}
 }
 
+/*
+ * A tracked reader takes 0x10000 reference counts on the block device buffer.
+ * b_count is not likely to reach 0x10000 by get_bh() calls, but even if it
+ * does, that will only affect the result of buffer_tracked_readers_count().
+ * After 0x10000 subsequent calls to get_bh_tracked_reader(), b_count will
+ * overflow, but that requires 0x10000 parallel readers from 0x10000 different
+ * snapshots and very slow disk I/O...
+ */
+#define BH_TRACKED_READERS_COUNT_SHIFT 16
+
+static inline void get_bh_tracked_reader(struct buffer_head *bdev_bh)
+{
+	atomic_add(1<<BH_TRACKED_READERS_COUNT_SHIFT, &bdev_bh->b_count);
+}
+
+static inline void put_bh_tracked_reader(struct buffer_head *bdev_bh)
+{
+	atomic_sub(1<<BH_TRACKED_READERS_COUNT_SHIFT, &bdev_bh->b_count);
+}
+
+static inline int buffer_tracked_readers_count(struct buffer_head *bdev_bh)
+{
+	return atomic_read(&bdev_bh->b_count)>>BH_TRACKED_READERS_COUNT_SHIFT;
+}
+
+extern int start_buffer_tracked_read(struct buffer_head *bh);
+extern void cancel_buffer_tracked_read(struct buffer_head *bh);
+extern int next3_read_full_page(struct page *page, get_block_t *get_block);
 
 #endif	/* _LINUX_NEXT3_SNAPSHOT_H */
